---
title: A Response To "Optimism corrected bootstrapping a problematic method"
author: Davis Vaughan
date: '2018-12-25'
slug: 'response-to-optimism'
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Introduction

![](https://img.shields.io/badge/post--status-maturing-blue.svg)

This is a quick rough draft of a response to [this](https://intobioinformatics.wordpress.com/2018/12/25/optimism-corrected-bootstrapping-a-problematic-method/) blog post, which I believe resulted in a faulty conclusion regarding the _optimism corrected bootstrap_ procedure. 

I believe their error stems mainly from the use of a _training_ data ROC AUC score, rather than _test_ data ROC AUC score. This comes from using `train()$results` rather than `train()$resample`.

Over the next few days, I will continue to refine this post, but I wanted to put this rough version up quickly to dispel any ideas that it was caret or the optimism corrected bootstrap that was the issue.

### Old code

This code comes straight from the blog post, except I've turned off the verbose output.

Notice that `max(fit3$results$ROC)` is used. This is the main problem.

```{r, cache = TRUE, message=FALSE, warning=FALSE}
library(caret)
allresults <- matrix(ncol=2,nrow=200)
i = 0
for (z in seq(10,2000,10)){
  
  i = i + 1
  
  # select only two species
  iris <- subset(iris, iris$Species == 'versicolor' | iris$Species == 'virginica')
  iris$Species <- droplevels(iris$Species)
  # generate random data
  test <- matrix(rnorm(100*z, mean = 0, sd = 1),
                 nrow = 100, ncol = z, byrow = TRUE)
  # bind random data
  iris <- cbind(iris,test)
  # remove real data
  iris <- iris[,-1:-4]
  
  # cross validation
  ctrl <- trainControl(method = 'cv',
                       summaryFunction=twoClassSummary,
                       classProbs=T,
                       savePredictions = T,
                       verboseIter = F)
  fit3 <- train(as.formula( paste( 'Species', '~', '.' ) ), data=iris,
                method="glmnet", # preProc=c("center", "scale")
                trControl=ctrl, metric = "ROC") #
  allresults[i,1] <- max(fit3$results$ROC)
  
  # optimism corrected bootstrapping
  ctrl <- trainControl(method = 'optimism_boot',
                       summaryFunction=twoClassSummary,
                       classProbs=T,
                       savePredictions = T,
                       verboseIter = F)
  fit4 <- train(as.formula( paste( 'Species', '~', '.' ) ), data=iris,
                method="glmnet", # preProc=c("center", "scale")
                trControl=ctrl, metric = "ROC") #
  allresults[i,2] <- max(fit4$results$ROC)
  
  rm(iris)
}

df <- data.frame(allresults)
colnames(df) <- c('cross_validation','optimism_corrected_boot')
df2 <- reshape2::melt(df)
df2$N <- c(seq(10,2000,10),seq(10,2000,10))

# do the plot
# p1 <- ggplot(df2, aes(x=N, y=value, group=variable)) +
#   geom_line(aes(colour=variable))
# png('bias_in_optimism_corrected_bootstrapping.png', height = 15, width = 27, units = 'cm',
#     res = 900, type = 'cairo')
# print(p1)
# dev.off()

ggplot(df2, aes(x=N, y=value, group=variable)) +
  geom_line(aes(colour=variable))
```

The above plot shows the number of predictor columns on the x-axis, and the ROC AUC value on the y-axis. The issue here is that it looks like the ROC AUC of the optimism bootstrap method increases as a function of the number of predictors, when it should really be flat at `0.5`.

### New code

This is a bit closer to what I would have done. Rewriting it helped me get to the bottom of the problem!

Here I use `mean(model_fit$resample$ROC)` to get the average of the resampled ROC AUC scores.

```{r, message=FALSE, warning=FALSE}
library(caret)
library(rlang)
library(purrr)
library(tibble)
library(tidyr)
library(ggplot2)

set.seed(123)

n_predictor_sets <- 200

min_n_predictors <- 10
max_n_predictors <- n_predictor_sets * 10
step <- 10

n_predictors_seq <- seq(from = min_n_predictors, to = max_n_predictors, by = step)

# select only two species and rip out Species as the outcome
iris <- subset(iris, iris$Species == 'versicolor' | iris$Species == 'virginica')
outcome_df <- data.frame(outcome = droplevels(iris$Species))

n_obs <- nrow(outcome_df)

# generate random data
generate_predictors <- function(n_predictors, n_obs) {
  
  n_random_points <- n_obs * n_predictors
  
  predictors <- matrix(
    data  = rnorm(n_random_points),
    nrow  = n_obs,
    ncol  = n_predictors,
  )
  
  predictors
}

all_predictors <- map(n_predictors_seq, generate_predictors, n_obs = n_obs)
all_data_sets <- map(all_predictors, cbind, outcome_df)

ctrl_cv <- trainControl(
  method          = 'cv',
  summaryFunction = twoClassSummary,
  classProbs      = TRUE,
  savePredictions = TRUE,
  verboseIter     = FALSE
)

ctrl_opt_boot <- trainControl(
  method          = 'optimism_boot',
  summaryFunction = twoClassSummary,
  classProbs      = TRUE,
  savePredictions = TRUE,
  verboseIter     = FALSE
)

model_formula <- outcome ~ .

fit_glmnet <- function(data_set, ctrl, formula) {
  model_fit <- train(
    form = formula, 
    data = data_set, 
    method = "glmnet", 
    trControl = ctrl, 
    metric = "ROC"
  )

  mean(model_fit$resample$ROC)
}
```

I use `furrr` to fit things in parallel so I'm not here all day.

```{r, cache=TRUE, message=FALSE, warning=FALSE}
library(furrr)

plan(multiprocess)

roc_cv <- future_map_dbl(
  all_data_sets, fit_glmnet, 
  ctrl = ctrl_cv, formula = model_formula
)

roc_opt_boot <- future_map_dbl(
  all_data_sets, fit_glmnet, 
  ctrl = ctrl_opt_boot, formula = model_formula
)
```

```{r, warning=FALSE, message=FALSE}
all_results <- tibble(
  cross_validation = roc_cv,
  optimism_corrected_boot = roc_opt_boot,
  n_predictors = n_predictors_seq
)

all_results <- gather(all_results, "method", "value", -n_predictors)
```

As you can see below, the ROC AUC scores computed while varying the number of predictors is actually more stable when using the optimism bootstrap, and the average is around the same value as the cross validation method.

```{r}
ggplot(all_results, aes(x = n_predictors, y = value, group = method)) +
  geom_line(aes(colour = method)) +
  labs(x = "Number of predictors", y = "ROC AUC")
```

```{r, warning=FALSE, message=FALSE}
library(dplyr)

all_results %>%
  group_by(method) %>%
  summarise(
    avg_roc_auc = mean(value),
    sd_roc_auc = sd(value)
  )
```

