---
title: A Response To "Optimism corrected bootstrapping a problematic method"
author: Davis Vaughan
date: '2018-12-25'
slug: 'response-to-optimism'
editor_options: 
  chunk_output_type: console
---



<div id="update-2018-12-29" class="section level3">
<h3>Update 2018-12-29</h3>
<p>After further reading into the guts of caret, I think my conclusions below are inaccurate, and the <code>train()$results</code> is fine to use. That being said, I would encourage you to look to Frank Harrell’s <a href="http://hbiostat.org/doc/simval.html">Comparison of Strategies</a> post that does a good job exploring various validation methods. One of the main takeaways is that the optimism bootstrap is fine when there are few predictors (or many predictors but with a <em>very</em> large data set, probably ~10x the number of predictors at least based on a few simulations of my own). However, when there are a sizable number of predictors in a small-ish data set (say, 60 predictors with 500 observations), the optimism bootstrap can be less competitive than a repeated cross validation method.</p>
<p>There is clearly more room for improvement in understanding the exact weaknesses of these methods.</p>
</div>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p><img src="https://img.shields.io/badge/post--status-maturing-blue.svg" /></p>
<p>This is a quick rough draft of a response to <a href="https://intobioinformatics.wordpress.com/2018/12/25/optimism-corrected-bootstrapping-a-problematic-method/">this</a> blog post, which I believe resulted in a faulty conclusion regarding the <em>optimism corrected bootstrap</em> procedure.</p>
<p>I believe their error stems mainly from the use of a <em>training</em> data ROC AUC score, rather than <em>test</em> data ROC AUC score. This comes from using <code>train()$results</code> rather than <code>train()$resample</code>.</p>
<p>Over the next few days, I will continue to refine this post, but I wanted to put this rough version up quickly to dispel any ideas that it was caret or the optimism corrected bootstrap that was the issue.</p>
</div>
<div id="old-code" class="section level3">
<h3>Old code</h3>
<p>This code comes straight from the blog post, except I’ve turned off the verbose output.</p>
<p>Notice that <code>max(fit3$results$ROC)</code> is used. This is the main problem.</p>
<pre class="r"><code>library(caret)
allresults &lt;- matrix(ncol=2,nrow=200)
i = 0
for (z in seq(10,2000,10)){
  
  i = i + 1
  
  # select only two species
  iris &lt;- subset(iris, iris$Species == &#39;versicolor&#39; | iris$Species == &#39;virginica&#39;)
  iris$Species &lt;- droplevels(iris$Species)
  # generate random data
  test &lt;- matrix(rnorm(100*z, mean = 0, sd = 1),
                 nrow = 100, ncol = z, byrow = TRUE)
  # bind random data
  iris &lt;- cbind(iris,test)
  # remove real data
  iris &lt;- iris[,-1:-4]
  
  # cross validation
  ctrl &lt;- trainControl(method = &#39;cv&#39;,
                       summaryFunction=twoClassSummary,
                       classProbs=T,
                       savePredictions = T,
                       verboseIter = F)
  fit3 &lt;- train(as.formula( paste( &#39;Species&#39;, &#39;~&#39;, &#39;.&#39; ) ), data=iris,
                method=&quot;glmnet&quot;, # preProc=c(&quot;center&quot;, &quot;scale&quot;)
                trControl=ctrl, metric = &quot;ROC&quot;) #
  allresults[i,1] &lt;- max(fit3$results$ROC)
  
  # optimism corrected bootstrapping
  ctrl &lt;- trainControl(method = &#39;optimism_boot&#39;,
                       summaryFunction=twoClassSummary,
                       classProbs=T,
                       savePredictions = T,
                       verboseIter = F)
  fit4 &lt;- train(as.formula( paste( &#39;Species&#39;, &#39;~&#39;, &#39;.&#39; ) ), data=iris,
                method=&quot;glmnet&quot;, # preProc=c(&quot;center&quot;, &quot;scale&quot;)
                trControl=ctrl, metric = &quot;ROC&quot;) #
  allresults[i,2] &lt;- max(fit4$results$ROC)
  
  rm(iris)
}

df &lt;- data.frame(allresults)
colnames(df) &lt;- c(&#39;cross_validation&#39;,&#39;optimism_corrected_boot&#39;)
df2 &lt;- reshape2::melt(df)
df2$N &lt;- c(seq(10,2000,10),seq(10,2000,10))

# do the plot
# p1 &lt;- ggplot(df2, aes(x=N, y=value, group=variable)) +
#   geom_line(aes(colour=variable))
# png(&#39;bias_in_optimism_corrected_bootstrapping.png&#39;, height = 15, width = 27, units = &#39;cm&#39;,
#     res = 900, type = &#39;cairo&#39;)
# print(p1)
# dev.off()

ggplot(df2, aes(x=N, y=value, group=variable)) +
  geom_line(aes(colour=variable))</code></pre>
<p><img src="/post/2018-12-25-response-to-optimism/index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The above plot shows the number of predictor columns on the x-axis, and the ROC AUC value on the y-axis. The issue here is that it looks like the ROC AUC of the optimism bootstrap method increases as a function of the number of predictors, when it should really be flat at <code>0.5</code>.</p>
</div>
<div id="new-code" class="section level3">
<h3>New code</h3>
<p>This is a bit closer to what I would have done. Rewriting it helped me get to the bottom of the problem!</p>
<p>Here I use <code>mean(model_fit$resample$ROC)</code> to get the average of the resampled ROC AUC scores.</p>
<pre class="r"><code>library(caret)
library(rlang)
library(purrr)
library(tibble)
library(tidyr)
library(ggplot2)

set.seed(123)

n_predictor_sets &lt;- 200

min_n_predictors &lt;- 10
max_n_predictors &lt;- n_predictor_sets * 10
step &lt;- 10

n_predictors_seq &lt;- seq(from = min_n_predictors, to = max_n_predictors, by = step)

# select only two species and rip out Species as the outcome
iris &lt;- subset(iris, iris$Species == &#39;versicolor&#39; | iris$Species == &#39;virginica&#39;)
outcome_df &lt;- data.frame(outcome = droplevels(iris$Species))

n_obs &lt;- nrow(outcome_df)

# generate random data
generate_predictors &lt;- function(n_predictors, n_obs) {
  
  n_random_points &lt;- n_obs * n_predictors
  
  predictors &lt;- matrix(
    data  = rnorm(n_random_points),
    nrow  = n_obs,
    ncol  = n_predictors,
  )
  
  predictors
}

all_predictors &lt;- map(n_predictors_seq, generate_predictors, n_obs = n_obs)
all_data_sets &lt;- map(all_predictors, cbind, outcome_df)

ctrl_cv &lt;- trainControl(
  method          = &#39;cv&#39;,
  summaryFunction = twoClassSummary,
  classProbs      = TRUE,
  savePredictions = TRUE,
  verboseIter     = FALSE
)

ctrl_opt_boot &lt;- trainControl(
  method          = &#39;optimism_boot&#39;,
  summaryFunction = twoClassSummary,
  classProbs      = TRUE,
  savePredictions = TRUE,
  verboseIter     = FALSE
)

model_formula &lt;- outcome ~ .

fit_glmnet &lt;- function(data_set, ctrl, formula) {
  model_fit &lt;- train(
    form = formula, 
    data = data_set, 
    method = &quot;glmnet&quot;, 
    trControl = ctrl, 
    metric = &quot;ROC&quot;
  )

  mean(model_fit$resample$ROC)
}</code></pre>
<p>I use <code>furrr</code> to fit things in parallel so I’m not here all day.</p>
<pre class="r"><code>library(furrr)

plan(multiprocess)

roc_cv &lt;- future_map_dbl(
  all_data_sets, fit_glmnet, 
  ctrl = ctrl_cv, formula = model_formula
)

roc_opt_boot &lt;- future_map_dbl(
  all_data_sets, fit_glmnet, 
  ctrl = ctrl_opt_boot, formula = model_formula
)</code></pre>
<pre class="r"><code>all_results &lt;- tibble(
  cross_validation = roc_cv,
  optimism_corrected_boot = roc_opt_boot,
  n_predictors = n_predictors_seq
)

all_results &lt;- gather(all_results, &quot;method&quot;, &quot;value&quot;, -n_predictors)</code></pre>
<p>As you can see below, the ROC AUC scores computed while varying the number of predictors is actually more stable when using the optimism bootstrap, and the average is around the same value as the cross validation method.</p>
<pre class="r"><code>ggplot(all_results, aes(x = n_predictors, y = value, group = method)) +
  geom_line(aes(colour = method)) +
  labs(x = &quot;Number of predictors&quot;, y = &quot;ROC AUC&quot;)</code></pre>
<p><img src="/post/2018-12-25-response-to-optimism/index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>library(dplyr)

all_results %&gt;%
  group_by(method) %&gt;%
  summarise(
    avg_roc_auc = mean(value),
    sd_roc_auc = sd(value)
  )</code></pre>
<pre><code>## # A tibble: 2 x 3
##   method                  avg_roc_auc sd_roc_auc
##   &lt;chr&gt;                         &lt;dbl&gt;      &lt;dbl&gt;
## 1 cross_validation              0.557     0.0929
## 2 optimism_corrected_boot       0.496     0.0650</code></pre>
</div>
